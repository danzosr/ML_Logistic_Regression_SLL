# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pi9JTXyS591YSmpp3gEl9szeFg1upBf2
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score
from sklearn.utils import resample
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Función para llenar los valores faltantes con media ± desviación estándar
def llenar_faltantes(row):
    for columna in df.columns:
        if pd.isnull(row[columna]):
            mean_val = estadisticas.loc['mean', columna]
            std_val = estadisticas.loc['std', columna]
            row[columna] = np.random.randint(mean_val - std_val, mean_val + std_val + 1)
    return row

"""
-- Limpieza de datos --
"""

# Importar el Dataset
df = pd.read_csv('framingham.csv')

# Calcular la cantidad y proporción de datos faltantes por columna
faltantes_por_columna = df.isnull().sum()
proporcion_faltantes_por_columna = df.isnull().mean()

# Graficar solo las columnas con datos faltantes
faltantesPorColumna = faltantes_por_columna[faltantes_por_columna > 0]
if not faltantesPorColumna.empty:
    fig = plt.figure(figsize=(11,4))
    plt.bar(faltantesPorColumna.index, faltantesPorColumna)
    plt.title('Cantidad de Datos Faltantes en las Features')
    plt.xlabel("Features")
    plt.ylabel("Instancias")

    for i, value in enumerate(faltantesPorColumna):
        plt.text(i, value + 5, str(value), ha='center')
    plt.show()

# Mostrar resultados
for columna in df.columns:
    faltantes = faltantes_por_columna[columna]
    proporcion = proporcion_faltantes_por_columna[columna]
    if faltantes > 0:
        print(f"Columna: {columna}")
        print(f"  - Cantidad de datos faltantes: {faltantes}")
        print(f"  - Proporción de datos faltantes: {proporcion:.2%}")
        print()

# Calcular la media y desviación estándar para las columnas con datos faltantes
estadisticas = df.agg(['mean', 'std'])

# Aplicar la función a cada fila del DataFrame
df = df.apply(llenar_faltantes, axis=1)

# Separar en X (features) e Y (label)
X = df.iloc[:, :-1].values
Y = df.iloc[:, -1].values

# Escalar manualmente las features en X
X_scaled = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

"""
-- Downsampling para balancear el dataset (50:50) --
"""

# Convertir a DataFrame para manejar más fácilmente el downsampling
df_balanced = pd.DataFrame(X_scaled)
df_balanced['label'] = Y

# Separar clases mayoritaria y minoritaria
df_negativos = df_balanced[df_balanced['label'] == 0]  # Clase mayoritaria (negativos)
df_positivos = df_balanced[df_balanced['label'] == 1]  # Clase minoritaria (positivos)

# Realizar downsampling de la clase mayoritaria
df_negativos_downsampled = resample(df_negativos,
                                    replace=False,    # sin reemplazo
                                    n_samples=len(df_positivos),  # Mismo tamaño que la clase positiva
                                    random_state=42)  # Fijar la semilla

# Combinar nuevamente las clases
df_downsampled = pd.concat([df_negativos_downsampled, df_positivos])

# Separar las features (X) y el label (Y)
X_downsampled = df_downsampled.iloc[:, :-1].values
Y_downsampled = df_downsampled.iloc[:, -1].values

# Dividir los datos en conjunto de entrenamiento y prueba
X_train, X_test, Y_train, Y_test = train_test_split(X_downsampled, Y_downsampled, test_size=0.2, random_state=42)

"""
-- Entrenamiento y evaluación del modelo --
"""

alphas = [0.0001, 0.001, 0.01, 0.1, 0.25, 0.5, 1.0, 10, 100, 250, 500, 1000, 2500]
epochs = 500
umbrales = np.arange(0.4, 0.61, 0.01)
resultados_completos = []
mejores_resultados_accuracy = []

for alpha in alphas:
    C_value = 1 / alpha  # C es inverso de alpha
    print(f"\nEntrenando modelo con alpha = {alpha} (C = {C_value:.2f}) y epochs = {epochs}")

    # Crear y entrenar el modelo con los valores de C y max_iter (epochs)
    log_reg = LogisticRegression(max_iter=epochs, C=C_value)
    log_reg.fit(X_train, Y_train)

    # Predicciones y probabilidad
    Y_pred = log_reg.predict(X_test)
    Y_pred_prob = log_reg.predict_proba(X_test)[:, 1]

    # Evaluar el modelo con diferentes umbrales
    mejores_resultados = []
    for umbral in umbrales:
        Y_pred_umbral = (Y_pred_prob >= umbral).astype(int)

        # Calcular las métricas
        conf_matrix = confusion_matrix(Y_test, Y_pred_umbral)
        precision = precision_score(Y_test, Y_pred_umbral)
        recall = recall_score(Y_test, Y_pred_umbral)
        accuracy = accuracy_score(Y_test, Y_pred_umbral)
        f1 = f1_score(Y_test, Y_pred_umbral)
        mejores_resultados.append((umbral, precision, recall, f1, accuracy))

    # Ordenar por F1 score
    mejores_resultados.sort(key=lambda x: x[3], reverse=True)
    mejores_para_alpha = mejores_resultados[0]  # Mejor resultado para este alpha
    # Ordenar por Accuracy
    mejores_resultados.sort(key=lambda x: x[4], reverse=True)
    mejores_para_accuracy = mejores_resultados[0]  # Mejor accuracy para este alpha

    # Almacenar los mejores resultados por alpha
    resultados_completos.append((alpha, mejores_para_alpha))

    mejores_resultados_accuracy.append((alpha, mejores_para_accuracy))

    # Mostrar los mejores resultados
    print(f"\nMejor resultado para alpha con F1 Score = {alpha} (C = {C_value:.2f}):")
    print(f"Umbral: {mejores_para_alpha[0]:.2f}, Precisión: {mejores_para_alpha[1]:.2f}, Recall: {mejores_para_alpha[2]:.2f}, F1 Score: {mejores_para_alpha[3]:.2f}, Accuracy: {mejores_para_alpha[4]:.2f}")

    print(f"\nMejor resultado para alpha con Accuracy = {alpha} (C = {C_value:.2f}):")
    print(f"Umbral: {mejores_para_accuracy[0]:.2f}, Precisión: {mejores_para_accuracy[1]:.2f}, Recall: {mejores_para_accuracy[2]:.2f}, F1 Score: {mejores_para_accuracy[3]:.2f}, Accuracy: {mejores_para_accuracy[4]:.2f}")


# Mostrar los mejores resultados generales
print("\n\nResultados finales para todos los alphas con F1 Score:")
for resultado in resultados_completos:
    alpha, (umbral, precision, recall, f1, accuracy) = resultado
    print(f"Alpha: {alpha}, Umbral: {umbral:.2f}, Precisión: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}, Accuracy: {accuracy:.2f}")


print("\n\nResultados finales para todos los alphas con Accuracy:")
for resultado in mejores_resultados_accuracy:
    alpha, (umbral, precision, recall, f1, accuracy) = resultado
    print(f"Alpha: {alpha}, Umbral: {umbral:.2f}, Precisión: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}, Accuracy: {accuracy:.2f}")

"""
# Conclusión

El mejor modelo para este caso particular sería uno que priorice la precisión sobre el recall, por el tipo de predicción que estamos haciendo, al tratarse de un tema de salud a futuro es mejor detectar falsos negativos a falsos positivos, por temas de demandas en la vida real, ya que es mejor optar por un x porcentaje de veracidad que de error.

Debido a lo anteriormente mencionado la metrica principal que se nos indica deberíamos escoger para el modelo debería ser F1 Score en lugar de Accuracy, esto debido a que esta metrica busca un equilibrio entre precision y recall, que es a lo que aspiramos, ya que una precisión del 100% es altamente propenso a ser muy ineficiente, detectando demasiados falsos negativos, aunque en este caso el mayor valor para F1 score no es el optimo que se presencia en los resultados, ya que en este caso como buscamos optimizar la precisión sin perjudicar en gran medida el resto de metricas, podríamos inclinarnos a un modelo con un valor para Aplha de 250 y un Umbral de 49, esto debido a que en este modelo se llega al mejor punto de equilibrio entre la presición del modelo y el resto de metricas, que aunque en efecto se tiene que sacrificar un poco la presición, esto aporta en gran medida al resto de metricas.

"""